if 1==1:  # Imports
    import PyPDF2
    import sys
    print(sys.executable)
    import os
    from langchain.chat_models import ChatOpenAI
    import streamlit as st
    from langchain.chains.question_answering import load_qa_chain
    from langchain.embeddings.openai import OpenAIEmbeddings
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.document_loaders import UnstructuredPDFLoader
    from langchain.vectorstores import Chroma
    import glob
    from io import StringIO
    from langchain.chains.summarize import load_summarize_chain

uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000, 
    chunk_overlap=20,
    # separators=[
    #     'Abstract',
    #     'Introduction',
    #     'Conclusions',
    #     '\n\n',
    #     '\n',
    #     ' ',
    #     '']
    )


if uploaded_file is not None:
    # bytes_data = uploaded_file.getvalue()
    # # st.write(bytes_data)
    # stringio = StringIO(bytes_data.decode())
    # st.write(stringio)
    if 1==1:  # Load and split text
        loader = UnstructuredPDFLoader(uploaded_file)
        def load_data():
            return loader.load()
    st.write("Loading data...")
    data = load_data()
#     # Remove the references section
    data[0].page_content = data[0].page_content.split('References')[0]
    # @st.cache_data
    def split_documents(_data):
        return text_splitter.split_documents(data)

    texts = split_documents(data)  # texts is a list 
    st.write(f"Number of documents: {len(texts)}")
        
    embeddings = OpenAIEmbeddings()

    from langchain.docstore.document import Document

    run_summarization = st.button("Run summarization")
    if run_summarization:
        llm = ChatOpenAI(temperature=0.5, max_tokens=400)
        chain = load_summarize_chain(llm, chain_type="map_reduce")
        st.write("Running summarization...")
        summary = chain.run(texts[:5])
        st.success(summary)
# db = Chroma.from_documents(texts, embeddings)

# def pretty_print(response):
#     import textwrap
#     # Split the response by lines of max 80 characters
#     return '\n'.join(textwrap.wrap(response, 80))


# with st.expander("Summary"):
#     st.write(summary)

# if 1==1:  # Sample questions
#     query = """
#     Think of 10 technical questions relevant to dam engineering one could ask about this context and return them with 
#     double spaces between them in a bullet list.
#     """
#     docs = db.similarity_search(query)
#     sample_questions = chain.run(input_documents=docs, question=query)
#     with st.expander("Sample questions"):
#         st.write(sample_questions)

# if 1==1:  # Display images
#     # display the image in streamlit
#     folder = pdf_file.split("\\")[-2]
#     file_name = pdf_file.split("\\")[-1].replace(".pdf", "")

#     image_files = glob.glob(f"images/{folder}/{file_name}/*.png")

#     # Filter image files to those with size > 100 KB

#     # Sort these files from largest to smallest
#     image_files.sort(key=os.path.getsize, reverse=True)

#     # Display 9 images in a 3 x 3 grid
#     with st.expander("Images from the paper"):
#         for i in range(3):
#             cols = st.columns(3)
#             for j in range(3):
#                 image_i= i*3 + j
#                 try:
#                     cols[j].image(image_files[image_i])
#                 except: 
#                     pass
            


# query = st.text_input("Ask a question of this PDF: (Puede ser en espa√±ol!)", "")
# if query != "":  # Ask questions of the paper
#     docs = db.similarity_search(query)
#     # docs = docsearch.similarity_search(query, include_metadata=True)
#     response = chain.run(input_documents=docs, question=query)
#     st.success(pretty_print(response))
#     e1 = st.expander("Relevant fragments from the PDF:")    
#     for i, doc in enumerate(docs):
#         e1.write(f"Relevant document # {i}:")
#         e1.write(doc.page_content.replace("\n\n", "\n "))

    
# e2 = st.expander("Full text head")
# e2.write(data[0].page_content.replace("\n\n", "\n ")[0:10000])
    